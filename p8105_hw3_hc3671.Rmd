---
title: "p8105_hw3_hc3671"
author: "Barry"
date: "2025-10-06"
output: github_document
---

## Problem 1

Loading data and set options:
```{r}
knitr::opts_chunk$set(
  fig.path   = "plots/",   
  fig.width  = 12, fig.height = 8,
  dpi        = 320,
  fig.retina = 2,            
  dev        = "png"       
)
```


```{r}
library(p8105.datasets)
library(ggridges)              
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

data("instacart")

instacart = janitor::clean_names(instacart)

instacart |> 
  count(department, name = "number") |>  # not gonna show results here for aes
  mutate(max_department = department[which.max(number)], min_department = department[which.min(number)])
num_customers = n_distinct(instacart$user_id)
mean_day_since_last_order = mean(instacart$days_since_prior_order)
max_dow = names(which.max(table(instacart$order_dow)))
min_dow = names(which.min(table(instacart$order_dow)))
total_product_number = sum(unique(instacart$product_id))
num_reorder = sum(instacart$reordered)
num_orderfirst = sum(1 - instacart$reordered)

```

I made a look at the whole data set, this is a data about information on grocery orders on the Instacart online grocery platform in the United States. It has 15 variables in total.
Key variables include:
`order_id`: Unique identifier for each order.
`user_id`: Identifier for the customer.
`product_id` and `product_name`: Identifier for the purchased item.
`department` and `aisle`: Categorical variables of the products.
`order_dow` and `order_hour_of_day`: Day of the week and hour of the orders.
`days_since_prior_order`: Number of days since the user’s previous order.
`reordered`: A binary indicator indicates whether the user has purchased this product before (0/1).

I made some basic statistical tests like: there are `r num_customers` customers and `r total_product_number` products involved in this data, the mean of the days_since_prior_order is `r mean_day_since_last_order`, the max order_dow is `r max_dow`, the minimum order_dow is `r min_dow`, the number of reodered cases is `r num_reorder`, while there are `r num_orderfirst` customers that order online for the first time.

Next, I am gonna answer the questions listed in question 1:

(a). How many aisles are there, and which aisles are the most items ordered from?

```{r echo = FALSE}

instacart |> 
  summarize(
    num_aisle = n_distinct(aisle),
    most_ordered_aisle = aisle[which.max(num_aisle)]
  )
```

There are 134 aisles, and yogurts are the most items.

(b). Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
instacart |> 
  count(aisle, name = "num_aisle") |> 
  filter(num_aisle >= 10000) |>  
  ggplot(aes(x = aisle, y = num_aisle)) +
  geom_col() + 
  coord_flip() +
  labs(
    x = "Aisle",
    y = "Number of items ordered",
    title = "the number of items ordered in each aisle "
  ) +
  scale_y_continuous(trans = "sqrt")
  
```

(c). Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r}

instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> 
  group_by(aisle) |> 
  count(product_name, name = "number_of_times", sort = TRUE) |> 
  mutate(numbers_ranking = min_rank(-number_of_times)) |> 
  filter(numbers_ranking %in% c(1, 2, 3))

```

(d). Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}

instacart |> 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |> 
  group_by(product_name, order_dow) |> 
  summarise(
    mean_hour_of_day = mean(order_hour_of_day)
  ) |> 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_of_day
  ) |> 
  set_names(c("Product_name", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
  

```

## Question 2

Import, clean, and otherwise tidy these datasets.

```{r}

price= 
  read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", na = "NA") |>
  janitor::clean_names() |> 
  mutate(
    county_name = str_remove(county_name, " County$")
  ) |>
  rename_with(~str_remove(.x, "^x"), .cols = "x2015_01_31" : "x2024_08_31") |> 
  rename(zip_code = region_name, county = county_name) |> 
  filter(!if_all(`2015_01_31`:`2024_08_31`, is.na)) |> 
  select(-c(region_type, state_name))

zip = 
  read_csv("zillow_data/Zip Codes.csv", na = "NA") |> 
  janitor::clean_names() 

```

(a). There are 116 months between January 2015 and August 2024. How many ZIP codes are observed 116 times? How many are observed fewer than 10 times? Why are some ZIP codes are observed rarely and others observed in each month?

```{r}

price |> 
  mutate(
    observed_time = rowSums(!is.na(across(`2015_01_31`:`2024_08_31`)))
  ) |> 
  summarise(
    num_all_observed = sum(observed_time == 116),
    num_observed_over_10 = sum(observed_time >= 10)
  )

```

There are 48 ZIP codes are observed 116 times. 123 of them are observed fewer than 10 times. \
some ZIP codes are observed rarely and others observed in each month may be because some ZIP codes are not used any more or some places that are not converged only have a limited data available in Zillow, or maybe some places were not a living area in the past.

(b). Create a reader-friendly table showing the average rental price in each borough and year (not month). Comment on trends in this table.

```{r}

bond_data = 
  left_join(
    price, 
    zip, 
    by = c("zip_code", "county")
  ) 

rent_change_by_year =  
  bond_data |> 
  pivot_longer(
    `2015_01_31`:`2024_08_31`,
    names_to = "date",
    values_to = "rent",
    values_drop_na = TRUE
  ) |>
  mutate(
    date = ymd(str_replace_all(date, "_", "-")),
    year = lubridate::year(date),
    borough = case_when(
      county == "New York"  ~ "Manhattan",
      county == "Kings"     ~ "Brooklyn",
      county == "Queens"    ~ "Queens",
      county == "Bronx"     ~ "The Bronx",
      county == "Richmond"  ~ "Staten Island"
    )
  )  
  
rent_change_by_year |> 
  group_by(borough, year) |> 
  summarize(
    mean_rent = mean(rent, na.rm = TRUE),
    .groups = "drop"
  ) |> 
  pivot_wider(
    names_from = year,
    values_from = mean_rent
  ) |>   
  mutate(
    across(where(is.numeric), ~ round(.x, 1)),
    across(everything(), ~ ifelse(is.na(.x), "-", .x)))

```

From 2015 to 2024, the rent in every borough shows an increasing trend, especially after 2021, which allies  with the Covid-19 pandemic. Plus, there is a decrease in Brooklyn, Manhattan and Queens around 2019 and 2020.
Manhattan's mean rent is always the highest among all, while the Bronx's is the lowest.

(c).Make a plot showing NYC Rental Prices within ZIP codes for all available years. Your plot should facilitate comparisons across boroughs. Comment on any significant elements of this plot.

```{r}

mean_price_2015_2024 =
  rent_change_by_year |>
  group_by(borough, year) |>
  summarize(mean_rent = mean(rent, na.rm = TRUE), .groups = "drop") |>
  ggplot(aes(x = year, y = mean_rent, color = borough)) +
  geom_point() + geom_line() +
  theme(legend.position = "bottom") +
  labs(title = "NYC Rental Prices Across Boroughs (2015–2024)", x = "Year", y = "Average Rent", )

```

I tried to make a plot that includes all the zip code points together, but it is really not readable, I think this plot is what it is supposed to be. From the plot, we can see the mean rents have increased across all the boroughs, but with a very clear dip in 2020 because of the pandemic, except the Bronx (Bronx had a dip in 2016). The Staten Island only have data from 2020, while others are from 2015 to 2024.

(d). Compute the average rental price within each ZIP code over each month in 2023. Make a reader-friendly plot showing the distribution of ZIP-code-level rental prices across boroughs; put differently, your plot should facilitate the comparison of the distribution of average rental prices across boroughs. Comment on this plot.

```{r}

rent_2023 =
  rent_change_by_year |>
  filter(year == 2023) |>
  mutate(month = lubridate::month(date)) |>
  group_by(zip_code, borough, year, month) |>
  summarise(mean_rent = mean(rent))


# without months
price2023_without_month <-
  rent_2023 |>
  ggplot(aes(x = mean_rent, fill = borough)) +
  geom_density(alpha = .4) +
  theme(legend.position = "bottom") +
  labs(title = "Distribution of ZIP-Code-Level Rental Prices by Borough (2023)", y = "Average Monthly Rent")


# include month
price2023_with_month <- 
rent_2023 |>
  mutate(month = factor(month, levels = 1:12, labels = month.name)) |>
  ggplot(aes(x = mean_rent, y = borough, fill = borough)) +
  geom_density_ridges(alpha = 0.7,
                      scale = 1.05,
                      color = NA) +
  facet_wrap( ~ month, ncol = 4) +
  labs(title = "Distribution of ZIP-level Average Monthly Rent by Borough (2023)", x = "Average Monthly Rent ($)", y = NULL) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none")

```
The two plots above each shows the overall distribution and distribution by months of the five boroughs. From the second plot, we can see that there are slight difference between months, then we just see the overall pattern. Staten Land, Bronx and Queens's rental prices are converged around 2200, Brooklyn's has a peak around 2800, Manhatten has the highest price, which is almost around 4200. Moreover, Staten Land, Bronx and Queens's rental prices are relatively concentrated. In contrast, Manhattan and Brooklyn show more dispersed distributions, suggesting greater within-borough variability in rental prices.

(e). Combine the two previous plots into a single graphic, and export this to a results folder in your repository.

```{r}
library(patchwork)
combine <- mean_price_2015_2024 + price2023_without_month +
  plot_annotation(
    title = "Combination",
  )
dir.create("results")
ggsave(
  filename = "results/combined_plot.png",
  plot = combine,
  width = 12,
  height = 8,
  bg = "white"
)
```

## Question 3

```{r}
paticipant_df = 
  read_csv("https://p8105.com/data/nhanes_covar.csv", skip = 4) 
accelerometer_df = 
  read_csv("https://p8105.com/data/nhanes_accel.csv")

nhanes = 
  left_join(
    paticipant_df,
    accelerometer_df,
    by = "SEQN"
  ) |> 
  drop_na() |> 
  filter(age >= 21) |> 
  mutate(
    age = as.integer(age),
    education = as.factor(education),
    sex = 
      case_match(
        sex, 
        1 ~ "male", 
        2 ~ "female")
  )
```

(a). Produce a reader-friendly table for the number of men and women in each education category, and create a visualization of the age distributions for men and women in each education category. Comment on these items.

```{r}
# plot of the number of men and women in each education category
nhanes |> 
  count(education, sex, name = "population") |> 
  pivot_wider(
    names_from = sex,
    values_from = population
  )

# visualization

nhanes |> 
  ggplot(aes(x = age, fill = sex)) +
  geom_density(alpha = .5) +
  facet_grid(~education) +
  viridis::scale_fill_viridis(discrete = TRUE)


```

From the plot, for education level 1, makes and females are basically the same. For education level 2, males have a larger part of the younger age than females, and for education level 3, females' population has a heavier density around age of 30.

(b). Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each participant. Plot these total activities (y-axis) against age (x-axis); your plot should compare men to women and have separate panels for each education level. Include a trend line or a smooth to illustrate differences. Comment on your plot.

```{r}
nhanes_activity  =
  nhanes |>
  mutate(total = rowSums(select(nhanes , min1:min1440), na.rm = TRUE))

nhanes_activity |> 
  ggplot(aes(x = age, y = total, color = sex)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, size = 1.2) +
  facet_grid(~ education) +
  labs(
    title = "Total Daily Activity vs Age by Sex and Educational level",
    x = "Age", y = "Total Daily Activity"
  ) 

```

For education level 1, female has more daily activity than male before 40, and then change after 40, with a peak for male aged 60. \
For education level 2, female' total daily activity is more than male's almost among all age, with a peak in 40 and 70 in both make and female. \
For education level 3, male and female's total daily activity pattern are almost the same. But the line shows that female still has more total daily activity than male.

(c). Accelerometer data allows the inspection activity over the course of the day. Make a three-panel plot that shows the 24-hour activity time courses for each education level and use color to indicate sex. Describe in words any patterns or conclusions you can make based on this graph; including smooth trends may help identify differences.

```{r}

nhanes |> 
  pivot_longer(
     min1:min1440,
     names_to = "time",
     values_to = "mims"
  ) |> 
  mutate(
    time = parse_number(time) / 1440 *24,
  ) |> 
  group_by(education, sex, time) |> 
  summarise(
    mean_mims = mean(mims)
  ) |> 
  ggplot(aes(x = time, y = mean_mims, color = sex)) +
  geom_point(alpha = .5, size = .3) +
  geom_smooth(se = FALSE) + 
  scale_x_continuous(
    limits = c(0, 24),
    breaks = seq(0, 24, by = 1)) +
  facet_grid(education ~.) +
  labs(
    title = "24-Hour Activity",
    x = "Time of Day (minute)",
    y = "Mean MIMS"
  )
  theme(legend.position = "bottom")

```

The general pattern (of all the educational level), the activity follows a clear circadian rhythm:
Lowest level during the night (roughly 0-6 am),
A sudden increase after waking (around 6–8 am),
Peak activity sustained throughout the daytime (10 am–5 pm),
Gradual decline into the evening hours (after 8 pm).

Then, let group 1,2,3 denotes different educational levels of people. Group 1 reaches the peak about two hours later than the other two groups. And group 3's pattern is flatter during the peak activity period than others and turns to decline sharper after work (around 7 pm). It seems that group two has a large variance of MIMS almost the daytime. And there is almost the same pattern of males and females.



